{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fan Sentiment Towards <i>The Marvelous Mrs. Maisel</i> based on Fan-Writen Works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Fanfiction (works written by fans of a book, television or online streaming program, movie, comic, etc.) often diverges significantly from the canon work and could be seen as what fans would prefer to have happened. These works may also show how people feel about characters from the canon by portraying the characters differently. For example, the are many fanfictions that show portray the <i>Harry Potter</i> characters Severus Snape and Draco Malfoy fondly, despite Snape being an antihero and Draco Malfoy largely an antagonist in J.K. Rowling's original work.</p>\n",
    "<p>This project seeks to assess fan sentiment towards the characters of Amazon Prime's 2017 to present <i>The Marvelous Mrs. Maisel</i>, using text mining with Python 3 and Voyant Tools to look at parts of speech and collocates. The works examined were all posted the fanfiction website Archive of Our Own (AO3), marked as complete by their authors, written in English, had a word count of at least one word (AO3 users can upload artworks or links to vieos as well), and are not crossovers (works containg characters or settings from multiple fictional worlds). This resulted in a small corpus (47 works), which likely leads to imprecise or inaccurate results. An ideal iteration of this project would use: \n",
    "<ul>\n",
    "<li>a significantly larger corpus of works</li> \n",
    "<li>a fandom with more clearly defined protangonists, antagonists, and antiheroes</li>\n",
    "<li>works from multiple sites hosting fanfiction\n",
    "<ul>\n",
    "<li>AO3</li>\n",
    "<li>fanfiction.net</li>\n",
    "<li>LiveJournal</li>\n",
    "<li>fandom specific sites</li>\n",
    "</ul></li></ul>\n",
    "However, due to time and technological constraints, the corpus of 47 works was analyzed for this project. The coding and some explaination of it used for scraping and text analysis can be found in <a href=\"https://nbviewer.jupyter.org/github/nlgauvreau/LLCU612/blob/master/Final%20Assignment/Appendix%201%E2%80%93Scraping%20Work.ipynb\">Appendix 1</a> and <a href=\"https://nbviewer.jupyter.org/github/nlgauvreau/LLCU612/blob/master/Final%20Assignment/Appendix%202%E2%80%93Text%20Analysis%20Work.ipynb\">Appendix 2</a>, respectively.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
